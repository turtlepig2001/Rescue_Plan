{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
    "\n",
    "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
    "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
    "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
    "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
    "    'survival rate' as the argument.\n",
    "\n",
    "    \"\"\"\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    random_tensor.floor_()  # binarize\n",
    "    output = x.div(keep_prob) * random_tensor\n",
    "    return output\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    r\"\"\" LayerNorm that supports two data formats: channels_last (default) or channels_first.\n",
    "    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with\n",
    "    shape (batch_size, height, width, channels) while channels_first corresponds to inputs\n",
    "    with shape (batch_size, channels, height, width).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\"):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape), requires_grad=True)\n",
    "        self.bias = nn.Parameter(torch.zeros(normalized_shape), requires_grad=True)\n",
    "        self.eps = eps\n",
    "        self.data_format = data_format\n",
    "        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n",
    "            raise ValueError(f\"not support data format '{self.data_format}'\")\n",
    "        self.normalized_shape = (normalized_shape,)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.data_format == \"channels_last\":\n",
    "            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
    "        elif self.data_format == \"channels_first\":\n",
    "            # [batch_size, channels, height, width]\n",
    "            mean = x.mean(1, keepdim=True)\n",
    "            var = (x - mean).pow(2).mean(1, keepdim=True)\n",
    "            x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "            x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
    "            return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    r\"\"\" ConvNeXt Block. There are two equivalent implementations:\n",
    "    (1) depthwise_conv -> LayerNorm (channels_first) -> 1x1 Conv -> GELU -> 1x1 Conv; all in (N, C, H, W)\n",
    "    (2) depthwise_conv -> Permute to (N, H, W, C); LayerNorm (channels_last) -> Linear -> GELU -> Linear; Permute back\n",
    "    We use (2) as we find it slightly faster in PyTorch\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        drop_rate (float): Stochastic depth rate. Default: 0.0\n",
    "        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, drop_rate=0., layer_scale_init_value=1e-6):\n",
    "        super().__init__()\n",
    "        self.depthwise_conv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim)  # depthwise conv\n",
    "        self.norm = LayerNorm(dim, eps=1e-6, data_format=\"channels_last\")\n",
    "        self.pointwise_conv1 = nn.Linear(dim, 4 * dim)  # pointwise/1x1 convs, implemented with linear layers\n",
    "        self.act = nn.GELU()\n",
    "        self.pointwise_conv2 = nn.Linear(4 * dim, dim)\n",
    "        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim,)),\n",
    "                                  requires_grad=True) if layer_scale_init_value > 0 else None\n",
    "        self.drop_path = DropPath(drop_rate) if drop_rate > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        shortcut = x\n",
    "        x = self.depthwise_conv(x)\n",
    "        x = x.permute(0, 2, 3, 1)  # [N, C, H, W] -> [N, H, W, C]\n",
    "        x = self.norm(x)\n",
    "        x = self.pointwise_conv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pointwise_conv2(x)\n",
    "        if self.gamma is not None:\n",
    "            x = self.gamma * x\n",
    "        x = x.permute(0, 3, 1, 2)  # [N, H, W, C] -> [N, C, H, W]\n",
    "\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvNeXt(nn.Module):\n",
    "    r\"\"\" ConvNeXt\n",
    "        A PyTorch impl of : `A ConvNet for the 2020s`  -\n",
    "          https://arxiv.org/pdf/2201.03545.pdf\n",
    "    Args:\n",
    "        in_chans (int): Number of input image channels. Default: 3\n",
    "        num_classes (int): Number of classes for classification head. Default: 1000\n",
    "        depths (tuple(int)): Number of blocks at each stage. Default: [3, 3, 9, 3]\n",
    "        dims (int): Feature dimension at each stage. Default: [96, 192, 384, 768]\n",
    "        drop_path_rate (float): Stochastic depth rate. Default: 0.\n",
    "        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n",
    "        head_init_scale (float): Init scaling value for classifier weights and biases. Default: 1.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_chans: int = 3, num_classes: int = 1000, depths: list = None,\n",
    "                 dims: list = None, drop_path_rate: float = 0., layer_scale_init_value: float = 1e-6,\n",
    "                 head_init_scale: float = 1.):\n",
    "        super().__init__()\n",
    "        self.downsample_layers = nn.ModuleList()  # stem and 3 intermediate downsampling conv layers\n",
    "        stem = nn.Sequential(nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4),\n",
    "                             LayerNorm(dims[0], eps=1e-6, data_format=\"channels_first\"))\n",
    "        self.downsample_layers.append(stem)\n",
    "\n",
    "        # 对应stage2-stage4前的3个downsample\n",
    "        for i in range(3):\n",
    "            downsample_layer = nn.Sequential(LayerNorm(dims[i], eps=1e-6, data_format=\"channels_first\"),\n",
    "                                             nn.Conv2d(dims[i], dims[i+1], kernel_size=2, stride=2))\n",
    "            self.downsample_layers.append(downsample_layer)\n",
    "\n",
    "        self.stages = nn.ModuleList()  # 4 feature resolution stages, each consisting of multiple blocks\n",
    "        dp_rates = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
    "        cur = 0\n",
    "        # 构建每个stage中堆叠的block\n",
    "        for i in range(4):\n",
    "            stage = nn.Sequential(\n",
    "                *[Block(dim=dims[i], drop_rate=dp_rates[cur + j], layer_scale_init_value=layer_scale_init_value)\n",
    "                  for j in range(depths[i])]\n",
    "            )\n",
    "            self.stages.append(stage)\n",
    "            cur += depths[i]\n",
    "\n",
    "        self.norm = nn.LayerNorm(dims[-1], eps=1e-6)  # final norm layer\n",
    "        self.head = nn.Linear(dims[-1], num_classes)\n",
    "        self.apply(self._init_weights)\n",
    "        self.head.weight.data.mul_(head_init_scale)\n",
    "        self.head.bias.data.mul_(head_init_scale)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "            nn.init.trunc_normal_(m.weight, std=0.2)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward_features(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        for i in range(4):\n",
    "            x = self.downsample_layers[i](x)\n",
    "            x = self.stages[i](x)\n",
    "\n",
    "        return self.norm(x.mean([-2, -1]))  # global average pooling, (N, C, H, W) -> (N, C)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.forward_features(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def convnext_tiny(num_classes: int):\n",
    "    # https://dl.fbaipublicfiles.com/convnext/convnext_tiny_1k_224_ema.pth\n",
    "    model = ConvNeXt(depths=[3, 3, 9, 3],\n",
    "                     dims=[96, 192, 384, 768],\n",
    "                     num_classes=num_classes)\n",
    "    return model\n",
    "\n",
    "\n",
    "def convnext_small(num_classes: int):\n",
    "    # https://dl.fbaipublicfiles.com/convnext/convnext_small_1k_224_ema.pth\n",
    "    model = ConvNeXt(depths=[3, 3, 27, 3],\n",
    "                     dims=[96, 192, 384, 768],\n",
    "                     num_classes=num_classes)\n",
    "    return model\n",
    "\n",
    "\n",
    "def convnext_base(num_classes: int):\n",
    "    # https://dl.fbaipublicfiles.com/convnext/convnext_base_1k_224_ema.pth\n",
    "    # https://dl.fbaipublicfiles.com/convnext/convnext_base_22k_224.pth\n",
    "    model = ConvNeXt(depths=[3, 3, 27, 3],\n",
    "                     dims=[128, 256, 512, 1024],\n",
    "                     num_classes=num_classes)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 带有前缀的state_dict文件\n",
    "state_dict_file = '/home/ubuntu/zhc/dddd/cv_convnext-base_image-classification_garbage/pytorch_model.pt'\n",
    "\n",
    "# 加载带有前缀的state_dict\n",
    "state_dict_with_prefix = torch.load(state_dict_file)['state_dict']\n",
    "\n",
    "# 去掉前缀并创建新的state_dict\n",
    "new_state_dict = {}\n",
    "prefix_to_remove = 'backbone.'\n",
    "for key, value in state_dict_with_prefix.items():\n",
    "    if key.startswith(prefix_to_remove):\n",
    "        new_key = key[len(prefix_to_remove):]  # 去掉前缀\n",
    "        new_state_dict[new_key] = value\n",
    "    elif key == 'norm.weight':\n",
    "        new_key = 'norm3.weight'  # 将\"norm.weight\"修改为\"norm3.weight\"\n",
    "        new_state_dict[new_key] = value\n",
    "    elif key == 'norm3.bias':\n",
    "        new_key = 'norm.bias'\n",
    "        new_state_dict[new_key] = value\n",
    "    elif key == 'head.fc.weight':\n",
    "        new_key = 'head.weight'\n",
    "        new_state_dict[new_key] = value\n",
    "    elif key == 'head.fc.bias':\n",
    "        new_key = 'head.bias'\n",
    "        new_state_dict[new_key] = value\n",
    "    else:\n",
    "        new_state_dict[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['norm.weight', 'norm.bias'], unexpected_keys=['norm3.weight', 'norm3.bias', 'head.fc.weight', 'head.fc.bias'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用新的state_dict加载模型\n",
    "model = convnext_base(265)  # 替换为实际的模型类\n",
    "# model.load_state_dict(new_state_dict,strict=False)\n",
    "model.load_state_dict(new_state_dict, strict=False)\n",
    "# 现在，model的权重应该与带有前缀的state_dict匹配，但不再包含前缀\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_names = ['input']\n",
    "# output_names = ['output']\n",
    " \n",
    "# x = torch.randn(1,3,32,32,requires_grad=True)\n",
    " \n",
    "# torch.onnx.export(model, x, 'qaq.onnx', input_names=input_names, output_names=output_names, verbose='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adver_attack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
